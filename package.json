{
  "name": "huggingface-vscode",
  "displayName": "llm-vscode",
  "description": "LLM powered development for VS Code",
  "version": "0.1.0",
  "publisher": "HuggingFace",
  "icon": "small_logo.png",
  "engines": {
    "vscode": "^1.82.0"
  },
  "galleryBanner": {
    "color": "#100f11",
    "theme": "dark"
  },
  "badges": [
    {
      "url": "https://img.shields.io/github/stars/huggingface/llm-vscode?style=social",
      "description": "Star llm-vscode on Github",
      "href": "https://github.com/huggingface/llm-vscode"
    },
    {
      "url": "https://img.shields.io/twitter/follow/huggingface?style=social",
      "description": "Follow Huggingface on Twitter",
      "href": "https://twitter.com/huggingface"
    }
  ],
  "homepage": "https://huggingface.co",
  "repository": {
    "url": "https://github.com/huggingface/llm-vscode.git",
    "type": "git"
  },
  "bugs": {
    "url": "https://github.com/huggingface/llm-vscode/issues"
  },
  "license": "Apache-2.0",
  "categories": [
    "Machine Learning",
    "Programming Languages"
  ],
  "keywords": [
    "code",
    "assistant",
    "ai",
    "llm",
    "development",
    "huggingface"
  ],
  "activationEvents": [
    "*"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "llm.afterInsert",
        "title": "Llm: After Insert"
      },
      {
        "command": "llm.login",
        "title": "Llm: Login"
      },
      {
        "command": "llm.logout",
        "title": "Llm: Logout"
      },
      {
        "command": "llm.attribution",
        "title": "Llm: Show Code Attribution"
      }
    ],
    "configuration": [
      {
        "title": "Llm",
        "properties": {
          "llm.enableGhostText": {
            "type": "boolean",
            "default": true,
            "description": "Enable \"ghost-text\" completions"
          },
          "llm.configTemplate": {
            "type": "string",
            "enum": [
              "bigcode/starcoder",
              "codellama/CodeLlama-13b-hf",
              "Phind/Phind-CodeLlama-34B-v2",
              "WizardLM/WizardCoder-Python-34B-V1.0",
              "Custom"
            ],
            "default": "bigcode/starcoder",
            "description": "Choose your model template from the dropdown."
          },
          "llm.modelIdOrEndpoint": {
            "type": "string",
            "default": "bigcode/starcoder",
            "description": "Supply huggingface model id (ex: `bigcode/starcoder`) or custom endpoint (ex: https://bigcode-large-xl.eu.ngrok.io/generate) to which requests will be sent. When huggingface model id is supplied, hugging face API inference will be used."
          },
          "llm.fillInTheMiddle.enabled": {
            "type": "boolean",
            "default": true,
            "description": "Enable fill in the middle for the current model"
          },
          "llm.fillInTheMiddle.prefix": {
            "type": "string",
            "default": "<fim_prefix>",
            "description": "Prefix token"
          },
          "llm.fillInTheMiddle.middle": {
            "type": "string",
            "default": "<fim_middle>",
            "description": "Middle token"
          },
          "llm.fillInTheMiddle.suffix": {
            "type": "string",
            "default": "<fim_suffix>",
            "description": "Suffix token"
          },
          "llm.temperature": {
            "type": "float",
            "default": 0.2,
            "description": "Sampling temperature"
          },
          "llm.maxNewTokens": {
            "type": "integer",
            "default": 60,
            "description": "Max number of new tokens to be generated. The accepted range is [50-500] both ends inclusive. Be warned that the latency of a request will increase with higher number."
          },
          "llm.contextWindow": {
            "type": "integer",
            "default": 8192,
            "description": "Context window of the model"
          },
          "llm.tokensToClear": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "default": [
              "<|endoftext|>"
            ],
            "description": "(Optional) Tokens that should be cleared from the resulting output. For example, in FIM mode, one usually wants to clear FIM token from resulting outout."
          },
          "llm.attributionWindowSize": {
            "type": "integer",
            "default": 250,
            "description": "Number of characters to scan for code attribution"
          },
          "llm.attributionEndpoint": {
            "type": "string",
            "default": "https://stack.dataportraits.org/overlap",
            "description": "Endpoint to which attribution request will be sent to (https://stack.dataportraits.org/overlap for the stack)."
          },
          "llm.tlsSkipVerifyInsecure": {
            "type": "boolean",
            "default": false,
            "description": "Skip TLS verification for insecure connections"
          },
          "llm.lsp.binaryPath": {
            "type": [
              "string",
              "null"
            ],
            "default": null,
            "description": "Path to llm-ls binary, useful for debugging or when building from source"
          },
          "llm.lsp.logLevel": {
            "type": "string",
            "default": "warn",
            "description": "llm-ls log level"
          },
          "llm.tokenizer": {
            "type": [
              "object",
              "null"
            ],
            "default": null,
            "description": "Tokenizer configuration for the model, check out the documentation for more details"
          }
        }
      }
    ],
    "keybindings": [
      {
        "key": "cmd+shift+l",
        "command": "editor.action.inlineSuggest.trigger"
      },
      {
        "key": "cmd+shift+a",
        "command": "llm.attribution"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js"
  },
  "dependencies": {
    "undici": "^5.25.2",
    "vscode-languageclient": "^8.1.0"
  },
  "devDependencies": {
    "@types/mocha": "^10.0.1",
    "@types/node": "16.x",
    "@types/vscode": "^1.82.0",
    "@typescript-eslint/eslint-plugin": "^6.4.1",
    "@typescript-eslint/parser": "^6.4.1",
    "@vscode/test-electron": "^2.3.4",
    "@vscode/vsce": "^2.21.0",
    "eslint": "^8.47.0",
    "glob": "^10.3.3",
    "mocha": "^10.2.0",
    "ovsx": "^0.8.3",
    "typescript": "^5.1.6"
  }
}